{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca1c99d1",
   "metadata": {},
   "source": [
    "# CMS Latin Exam Lemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2114512",
   "metadata": {},
   "source": [
    "To do:\n",
    "\n",
    "- biggest issue right now is that Latin WordNet's `lemmatize/` is defunct. Probably solve by cloning https://github.com/latinwordnet/latinwordnet-archive . But if it comes back online probably `from latinwordnet import LatinWordNet()` to save you some syntax.\n",
    "- other biggest issue is that I have no way of selecting the correct (usually the most frequent) lexeme from among the lemmatizer's hits, probably solve by querying some online frequency database. But ideally I would address that after regaining access to Latin WordNet.\n",
    "- not sure why it's assigning `ymago` when my manual list clearly refers that form to `imago`\n",
    "- how best to address the fact that at least one of my lemmatizers assigns infinitives while others don't? Probably run all NLTK results through Latin WordNet, once it comes back online."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3243a96f-1802-4457-b277-4c8f5353e640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,glob,re,json,requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import word_tokenize\n",
    "from CustomLatinBackoffLemmatizer import CustomLatinBackoffLemmatizer\n",
    "lem = CustomLatinBackoffLemmatizer()\n",
    "session = requests.Session()\n",
    "retry = Retry(connect=10, backoff_factor=0.5)\n",
    "adapter = HTTPAdapter(max_retries=retry)\n",
    "session.mount('http://', adapter)\n",
    "session.mount('https://', adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d0a2813",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization = [\n",
    "    (\"ę\", \"ae\"),\n",
    "    (\"æ\", \"ae\"),\n",
    "    (\"j\", \"i\")\n",
    "    ]\n",
    "\n",
    "def normalize(string):\n",
    "    # In addition to normalizing the above set, we'll print consonantal <u> as <v>:\n",
    "    # TO DO: ensure reuuixit is correctly normalized without affecting Erkenuualdus\n",
    "    for (k,v) in normalization:\n",
    "        string = string.replace(k,v)\n",
    "        string = re.sub('(?<=[aeiouy])u(?=[aeiouy])', 'v', string)\n",
    "        string = re.sub('^u(?=[aeiouy])', 'v', string)\n",
    "    return string    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "219d1160-75bb-42c3-b94e-2be392e75521",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# Strip an exam of its preamble and store exam identifier, passages, and their titles in a dict:\n",
    "def isolate(doc):\n",
    "    raw = open(doc).read()\n",
    "    startpos = re.search('\\n1\\.\\s*', raw).start()\n",
    "    content = raw[startpos:]\n",
    "    titles = []\n",
    "    title_start_pos = []\n",
    "    title_end_pos = []\n",
    "    for i in range(1,5):\n",
    "        # Store titles 1--4 in a list, and also remember their start and end indices;\n",
    "        # those tell us where the text of the passages begins and ends, too:\n",
    "        title = re.search(rf'(?<=\\n){i}\\.\\ .*(?=\\n)', content)\n",
    "        titles.append(title.group(0))\n",
    "        title_start_pos.append(title.start())\n",
    "        title_end_pos.append(title.end())\n",
    "    # Add a value of -1 to the list of start positions so the following loop works for the final text, too:\n",
    "    title_start_pos.append(-1)\n",
    "    # Now store the text of the four passages in a list:\n",
    "    texts = []\n",
    "    for i in range(0,4):\n",
    "        texts.append(content[title_end_pos[i]:title_start_pos[i+1]].strip('\\n '))\n",
    "    exam = {}\n",
    "    exam['id'] = doc.split('/')[1].removesuffix('.txt')\n",
    "    exam['titles'] = titles\n",
    "    exam['passages'] = texts\n",
    "    return exam\n",
    "\n",
    "# Run the above function on a corpus and return the result as a list:\n",
    "def load_corpus(data_set):\n",
    "    corpus = []\n",
    "    for file in sorted(glob.glob('exams/{}*txt'.format(data_set))):\n",
    "        corpus.append(isolate(file))\n",
    "    return corpus\n",
    "\n",
    "# Test for the existence of named-entities lists, else generate on the basis of capitalization and punctuation\n",
    "# (this required me to tweak corpus capitalization in accordance with my particular NER specifications):\n",
    "def ner_generate(data_sets):\n",
    "    all_ner_lists = dict()\n",
    "    for corpus in data_sets:\n",
    "        if os.path.exists('ner_{}.json'.format(corpus[0])):\n",
    "            entities = json.load(open('ner_{}.json'.format(corpus[0])))\n",
    "        else:\n",
    "            entities = []\n",
    "            for exam in corpus[1]:\n",
    "                tokens = word_tokenize(' '.join(exam['passages']))\n",
    "                counter = 0\n",
    "                for token in tokens:\n",
    "                    if token[0].isupper():\n",
    "                        if tokens[counter-1][-1].isalpha() or tokens[counter-1][-1] == ',':\n",
    "                            entities.append(normalize(token.lower().strip('.')))\n",
    "                    counter += 1\n",
    "            entities = sorted(list(set(entities)))\n",
    "            with open('ner_{}.json'.format(corpus[0]), 'w') as f:\n",
    "                json.dump(entities, f)\n",
    "        all_ner_lists[corpus[0]] = entities\n",
    "    return all_ner_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3ce4789-78a2-4a53-91b6-1b89d8c514e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(corpus):\n",
    "    if os.path.isfile('manual_ner_{}.txt'.format(corpus[0])):\n",
    "        ner_supplement = open('manual_ner_{}.txt'.format(corpus[0])).read().splitlines()\n",
    "    else:\n",
    "        ner_supplement = []\n",
    "    if os.path.isfile('lemmata_manual_{}.txt'.format(corpus[0])):\n",
    "        lemma_file = open('lemmata_manual_{}.txt'.format(corpus[0])).read().splitlines()\n",
    "        lemma_supplement = dict(line.split(' : ', 1) for line in lemma_file)\n",
    "    else:\n",
    "        lemma_supplement = dict()\n",
    "    cltk_results = {}\n",
    "    unrecognized = []\n",
    "    for exam in corpus[1]:\n",
    "        outfile = 'lemmata/{}.json'.format(exam['id'])\n",
    "        if not os.path.isfile(outfile):\n",
    "            lemmatized = []\n",
    "            for text in exam['passages']:\n",
    "                identified = []\n",
    "                tokens = tokenizer.tokenize(text)\n",
    "                for token in tokens:\n",
    "                    token = normalize(token.lower())\n",
    "                    ner_count = 0\n",
    "                    if token in lemma_supplement:\n",
    "                        identified.append(lemma_supplement[token])\n",
    "                    elif token in ner[corpus[0]] or token in ner_supplement:\n",
    "                        identified.append('NAME')\n",
    "                        ner_count += 1\n",
    "                    else:\n",
    "                        # https://latinwordnet.exeter.ac.uk/lemmatize/{} is defunct!\n",
    "                        r = session.get('https://latinwordnet.exeter.ac.uk/lemmatize/{}'.format(token))\n",
    "                        latinwordnet_result = r.json()\n",
    "                        if len(latinwordnet_result) == 0:\n",
    "                            if 'pre' in token:\n",
    "                                # Normalize \"pre\"-spellings:\n",
    "                                r = session.get('https://latinwordnet.exeter.ac.uk/lemmatize/{}'.format(token.replace('pre', 'prae')))\n",
    "                                result = r.json()\n",
    "                            elif 'y' in token:\n",
    "                                # Normalize \"y\"-spellings:\n",
    "                                r = session.get('https://latinwordnet.exeter.ac.uk/lemmatize/{}'.format(token.replace('y', 'i')))\n",
    "                                latinwordnet_result = r.json()\n",
    "                        if len(latinwordnet_result) == 0:\n",
    "                            cltk_result = lem.lemmatize([token])\n",
    "                            if cltk_result[0][1] == None:\n",
    "                                if 'pre' in token:\n",
    "                                    cltk_result = lem.lemmatize([token.replace('pre', 'prae')])\n",
    "                            if cltk_result[0][1] == None:\n",
    "                                if 'y' in token:\n",
    "                                    cltk_result = lem.lemmatize([token.replace('y', 'i')])\n",
    "                            if cltk_result[0][1] == None:\n",
    "                                unrecognized.append(token)\n",
    "                                identified.append('')\n",
    "                            else:\n",
    "                                identified.append(cltk_result[0][1])\n",
    "                        else:\n",
    "                            # Normalizing lexical forms because some lemmatizers normalize to consonantal <u>:\n",
    "                            identified.append(normalize(latinwordnet_result[[0]['lemma']['lemma']).rstrip('0123456789'))\n",
    "                lemmatized.append(identified)\n",
    "            print(sorted(list(set(unrecognized))))\n",
    "            with open(outfile, 'w') as f:\n",
    "                json.dump(lemmatized, f)\n",
    "            if os.path.isfile('lemmata/unidentified_{}.json'.format(corpus[0])):\n",
    "                unidentified = sorted(list(set(unrecognized + json.load(open('lemmata/unidentified_{}.json'.format(corpus[0]))))))\n",
    "            else:\n",
    "                unidentified = sorted(list(set(unrecognized)))\n",
    "            with open('lemmata/unidentified_{}.json'.format(corpus[0]), 'w') as f:\n",
    "                json.dump(unidentified, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b24dbdfa",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m data_sets \u001b[38;5;241m=\u001b[39m ((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml1\u001b[39m\u001b[38;5;124m'\u001b[39m, load_corpus(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml1\u001b[39m\u001b[38;5;124m'\u001b[39m)), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml2\u001b[39m\u001b[38;5;124m'\u001b[39m, load_corpus(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml2\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[1;32m      2\u001b[0m ner \u001b[38;5;241m=\u001b[39m ner_generate(data_sets)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mlemmatize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_sets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# set to [0] to lemmatize the Level One set, [1] for Level Two\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 61\u001b[0m, in \u001b[0;36mlemmatize\u001b[0;34m(corpus)\u001b[0m\n\u001b[1;32m     56\u001b[0m                     identified\u001b[38;5;241m.\u001b[39mappend(cltk_result[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     57\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m                 \u001b[38;5;66;03m# Normalizing lexical forms because some lemmatizers normalize to consonantal <u>:\u001b[39;00m\n\u001b[1;32m     59\u001b[0m                 \u001b[38;5;66;03m# (Replacing lemmatize/ with api/lemmas/ because the former is defunct)\u001b[39;00m\n\u001b[1;32m     60\u001b[0m                 \u001b[38;5;66;03m# identified.append(normalize(latinwordnet_result[[0]['lemma']['lemma']).rstrip('0123456789'))\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m                 identified\u001b[38;5;241m.\u001b[39mappend(normalize(\u001b[43mlatinwordnet_result\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresults\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlemma\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0123456789\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     62\u001b[0m     lemmatized\u001b[38;5;241m.\u001b[39mappend(identified)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(unrecognized))))\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "data_sets = (('l1', load_corpus('l1')), ('l2', load_corpus('l2')))\n",
    "ner = ner_generate(data_sets)\n",
    "lemmatize(data_sets[0]) # set to [0] to lemmatize the Level One set, [1] for Level Two"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
